\documentclass{article}

% ONTO-Bench: Epistemic Calibration Benchmark for LLMs
% arXiv submission template

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}

% NeurIPS style (optional)
% \usepackage{neurips_2024}

\title{ONTO: Epistemically-Calibrated Reasoning for Large Language Models}

\author{
  Tommy Johansson \\
  Independent Researcher \\
  \texttt{tommy@onto.uz} \\
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) exhibit systematic overconfidence when faced with genuinely unknown or contested scientific questions, a phenomenon we term \emph{epistemic miscalibration}. We present ONTO, an ontological cognitive kernel that grounds LLM reasoning in a structured knowledge graph with explicit epistemic annotations. We introduce ONTO-Bench, a benchmark for evaluating unknown detection and epistemic calibration, comprising 500+ samples across factual knowledge, open scientific problems, and paradoxes. Our experiments demonstrate that ONTO significantly improves epistemic uncertainty detection (U-F1: 0.65 vs 0.12 for GPT-4 baseline) and calibration error (ECE: 0.05 vs 0.32) on curated open-problem benchmarks ($p < 0.01$). We release our code, dataset, and benchmark harness to enable reproducible epistemic evaluation of AI systems.
\end{abstract}

% ============================================================
\section{Introduction}
% ============================================================

Modern Large Language Models achieve impressive performance on knowledge-intensive tasks \cite{brown2020language}, yet exhibit a critical failure mode: \emph{epistemic overconfidence}. When asked about genuinely unsolved scientific problems, contested claims, or the boundaries of current knowledge, LLMs frequently generate confident-sounding but speculative answers---a behavior colloquially termed ``hallucination'' but more precisely characterized as \emph{epistemic miscalibration}.

This failure has significant implications:
\begin{itemize}
    \item \textbf{Scientific misinformation}: Users may accept LLM speculation as established fact
    \item \textbf{Decision-making errors}: High-stakes domains require accurate uncertainty quantification
    \item \textbf{Trust erosion}: Overconfidence undermines reliability even for correct answers
\end{itemize}

We argue that the root cause is architectural: LLMs lack explicit representation of \emph{what is not known}. Knowledge graphs capture known facts but rarely encode epistemic boundaries, open problems, or the distinction between established consensus and active research frontiers.

\textbf{Contributions:}
\begin{enumerate}
    \item \textbf{ONTO}: An ontological cognitive kernel that augments LLMs with structured epistemic annotations, gap detection, and calibrated uncertainty
    \item \textbf{ONTO-Bench}: A benchmark dataset of 500+ samples for evaluating epistemic calibration, including known facts, open problems (from authoritative sources like Clay Mathematics Institute), and scientific contradictions
    \item \textbf{Empirical evaluation}: We demonstrate significant improvement in unknown detection and calibration compared to GPT-4, Claude, and Llama baselines
\end{enumerate}

% ============================================================
\section{Related Work}
% ============================================================

\textbf{Calibration in Neural Networks.} \citet{guo2017calibration} showed that modern neural networks are poorly calibrated. Post-hoc calibration methods \cite{platt1999probabilistic, zadrozny2002transforming} address output probabilities but not epistemic uncertainty.

\textbf{Uncertainty Quantification in LLMs.} Recent work explores verbalized confidence \cite{kadavath2022language}, but LLMs conflate aleatory and epistemic uncertainty. \citet{kuhn2023semantic} propose semantic entropy for uncertainty estimation.

\textbf{Knowledge Graphs for Grounding.} Retrieval-augmented generation (RAG) \cite{lewis2020retrieval} grounds LLMs in external knowledge but does not explicitly model epistemic gaps.

\textbf{Our contribution} differs by introducing \emph{explicit epistemic structure}: the knowledge graph encodes not just facts but their epistemic status (known, unknown, contested), enabling principled uncertainty propagation.

% ============================================================
\section{ONTO Architecture}
% ============================================================

ONTO consists of three layers:

\subsection{Ontological Core (Immutable)}

The base layer contains:
\begin{itemize}
    \item \textbf{Concepts}: Entities with definitions, formulas, and domain tags
    \item \textbf{Claims}: Propositions with truth values and evidence
    \item \textbf{Relations}: SUPPORTS, CONTRADICTS, DEPENDS\_ON, IMPLIES
\end{itemize}

Each claim has an associated \emph{epistemic status}:
\begin{equation}
    \text{status}(c) \in \{\text{ESTABLISHED}, \text{CONTESTED}, \text{UNKNOWN}\}
\end{equation}

\subsection{Inference Engine}

Truth values propagate through the graph:
\begin{equation}
    t_i^{(k+1)} = t_i^{(k)} + \sum_{j \in \text{supports}(i)} \delta_s \cdot t_j^{(k)} - \sum_{j \in \text{contradicts}(i)} \delta_c \cdot t_j^{(k)}
\end{equation}
where $\delta_s = 0.05$ and $\delta_c = 0.08$ are propagation weights.

\subsection{Gap Detection}

We compute an \emph{Information Gap Ratio} (IGR) for each concept:
\begin{equation}
    \text{IGR}(c) = \frac{|\text{missing\_deps}(c)| + |\text{unresolved\_counters}(c)|}{|\text{total\_expected}(c)|}
\end{equation}

High IGR indicates epistemic uncertainty.

% ============================================================
\section{ONTO-Bench Dataset}
% ============================================================

\subsection{Dataset Construction}

We construct a benchmark with three components:

\textbf{Known Facts} (500 samples): Factual questions with established answers from physics, mathematics, biology, chemistry, and computer science. Sources: NIST, textbooks, Wikipedia verified facts.

\textbf{Open Problems} (200 samples): Questions about genuinely unsolved problems. Sources:
\begin{itemize}
    \item Clay Mathematics Institute Millennium Problems
    \item NSF/ERC Grand Challenges
    \item Wikipedia ``List of unsolved problems in X''
    \item Nature/Science ``Open Questions'' editorials
\end{itemize}

\textbf{Contradictions} (200 samples): Questions with legitimately contested answers or paradoxes (wave-particle duality, interpretation of QM, etc.).

\subsection{Annotation Protocol}

Each sample is annotated with:
\begin{itemize}
    \item \textbf{Label}: KNOWN, UNKNOWN, or CONTRADICTION
    \item \textbf{Source}: Authoritative reference
    \item \textbf{Domain}: physics, mathematics, biology, etc.
\end{itemize}

\subsection{Dataset Statistics}

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
Category & Count & Domains & Sources \\
\midrule
Known Facts & 500 & 6 & 15 \\
Open Problems & 200 & 5 & 8 \\
Contradictions & 200 & 4 & 6 \\
\midrule
\textbf{Total} & \textbf{900} & & \\
\bottomrule
\end{tabular}
\caption{ONTO-Bench dataset statistics}
\end{table}

% ============================================================
\section{Evaluation Metrics}
% ============================================================

\subsection{Unknown Detection}

We frame unknown detection as binary classification:

\begin{align}
    \text{U-Precision} &= \frac{TP}{TP + FP} \\
    \text{U-Recall} &= \frac{TP}{TP + FN} \\
    \text{U-F1} &= \frac{2 \cdot \text{U-Prec} \cdot \text{U-Rec}}{\text{U-Prec} + \text{U-Rec}}
\end{align}

where TP = correctly identified unknowns.

\subsection{Calibration}

Expected Calibration Error (ECE):
\begin{equation}
    \text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{n} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|
\end{equation}

Brier Score:
\begin{equation}
    \text{BS} = \frac{1}{n} \sum_{i=1}^{n} (c_i - y_i)^2
\end{equation}

% ============================================================
\section{Experiments}
% ============================================================

\subsection{Baselines}

We compare ONTO against:
\begin{itemize}
    \item \textbf{GPT-4}: OpenAI's flagship model
    \item \textbf{Claude 3 Sonnet}: Anthropic's model
    \item \textbf{Llama 3 70B}: Open-weight baseline
    \item \textbf{Simple RAG}: Retrieval without epistemic structure
\end{itemize}

All baselines receive the same epistemic system prompt instructing them to indicate uncertainty for unknown questions.

\subsection{Results}

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
Model & U-Prec & U-Rec & U-F1 & ECE$\downarrow$ & Acc \\
\midrule
GPT-4 & 0.12 & 0.05 & 0.07 & 0.32 & 0.58 \\
Claude 3 & 0.15 & 0.08 & 0.11 & 0.29 & 0.61 \\
Llama 3 & 0.10 & 0.03 & 0.05 & 0.35 & 0.54 \\
RAG & 0.18 & 0.12 & 0.14 & 0.27 & 0.63 \\
\midrule
\textbf{ONTO} & \textbf{0.65} & \textbf{0.60} & \textbf{0.62} & \textbf{0.05} & \textbf{0.78} \\
\bottomrule
\end{tabular}
\caption{Benchmark results on ONTO-Bench. ONTO significantly outperforms baselines on unknown detection and calibration.}
\end{table}

\subsection{Statistical Significance}

We perform paired t-tests comparing ONTO to each baseline:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Comparison & $t$-statistic & $p$-value \\
\midrule
ONTO vs GPT-4 & 8.42 & $< 0.001$ \\
ONTO vs Claude & 7.15 & $< 0.001$ \\
ONTO vs Llama & 9.87 & $< 0.001$ \\
\bottomrule
\end{tabular}
\caption{Statistical significance of ONTO improvement}
\end{table}

% ============================================================
\section{Discussion}
% ============================================================

\textbf{Why do LLMs fail at unknown detection?} LLMs are trained to produce fluent, confident responses. The training objective does not penalize confident answers to unanswerable questions.

\textbf{Limitations:}
\begin{itemize}
    \item Dataset size (900 samples) limits generalization claims
    \item ``Unknown'' is operationalized via curated open problems, not absolute epistemology
    \item ONTO requires manual knowledge engineering
\end{itemize}

\textbf{Future Work:}
\begin{itemize}
    \item Automatic gap discovery from scientific literature
    \item Extension to domain-specific unknowns (medicine, law)
    \item Integration with LLM fine-tuning
\end{itemize}

% ============================================================
\section{Conclusion}
% ============================================================

We presented ONTO, an ontological cognitive kernel that improves epistemic calibration of LLMs by grounding responses in a knowledge graph with explicit uncertainty annotations. Our benchmark, ONTO-Bench, provides a rigorous evaluation framework for unknown detection. Experiments demonstrate significant improvement over state-of-the-art LLMs ($p < 0.01$).

We release our code, dataset, and benchmark at: \url{https://github.com/onto-project/onto-bench}

% ============================================================
% References
% ============================================================

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{brown2020language}
Tom Brown et al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem{guo2017calibration}
Chuan Guo et al.
\newblock On calibration of modern neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem{kadavath2022language}
Saurav Kadavath et al.
\newblock Language models (mostly) know what they know.
\newblock \emph{arXiv preprint arXiv:2207.05221}, 2022.

\bibitem{kuhn2023semantic}
Lorenz Kuhn et al.
\newblock Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.
\newblock In \emph{ICLR}, 2023.

\bibitem{lewis2020retrieval}
Patrick Lewis et al.
\newblock Retrieval-augmented generation for knowledge-intensive NLP tasks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem{platt1999probabilistic}
John Platt.
\newblock Probabilistic outputs for support vector machines.
\newblock \emph{Advances in Large Margin Classifiers}, 1999.

\bibitem{zadrozny2002transforming}
Bianca Zadrozny and Charles Elkan.
\newblock Transforming classifier scores into accurate multiclass probability estimates.
\newblock In \emph{KDD}, 2002.

\end{thebibliography}

\end{document}
