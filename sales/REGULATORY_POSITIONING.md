# ONTO Regulatory Positioning Pack

## Executive Summary

ONTO-Bench provides the **only standardized benchmark for epistemic calibration**—measuring whether AI systems recognize the boundaries of their knowledge. This document positions ONTO within emerging AI regulatory frameworks.

---

## Regulatory Landscape 2024-2026

### EU AI Act (Effective 2024-2026)

**Status:** In force. Enforcement phased in through 2026.

**Key Requirements:**

| Requirement | ONTO Alignment |
|-------------|----------------|
| Risk assessment for high-risk AI | ✓ ONTO Risk Score provides quantified assessment |
| Documentation of AI limitations | ✓ ONTO Report documents knowledge boundaries |
| Human oversight provisions | ✓ Unknown detection enables appropriate escalation |
| Transparency about AI capabilities | ✓ Compliance Score enables clear communication |

**Fines:**
- Up to €35M or 7% of global annual turnover (highest tier)
- Up to €15M or 3% for high-risk system violations
- Epistemic failures likely fall under "safety" requirements

**ONTO Value Proposition:**
> "ONTO provides audit-ready documentation of AI epistemic limits as required by EU AI Act Article 9 (Risk Management) and Article 13 (Transparency)."

---

### NIST AI Risk Management Framework (AI RMF)

**Status:** Published 2023. Voluntary framework, increasingly referenced in procurement.

**Key Functions:**

| NIST Function | ONTO Mapping |
|---------------|--------------|
| GOVERN: Establish risk tolerance | ONTO Compliance Score defines thresholds |
| MAP: Identify AI risks | ONTO Gap Detection identifies epistemic risks |
| MEASURE: Quantify risks | ONTO Metrics (ECE, U-Recall) provide measurements |
| MANAGE: Prioritize and act | ONTO Recommendations guide remediation |

**ONTO Value Proposition:**
> "ONTO operationalizes NIST AI RMF MEASURE function with standardized epistemic calibration metrics."

---

### US Federal Developments

**Executive Order on AI (Oct 2023)**
- Requires safety testing for powerful AI
- Epistemic calibration = safety metric

**FTC AI Enforcement**
- Increased scrutiny of AI claims
- "AI that doesn't know its limits" = deceptive practice risk

**SEC AI Disclosure**
- Public companies must disclose material AI risks
- Epistemic failures are material risks

**State Laws**
- California AI transparency bills
- NYC AI bias audit requirements (analogous model)

---

### Healthcare AI (FDA)

**FDA AI/ML Guidance:**
- "Predetermined change control plans" for AI
- Must document AI limitations
- ONTO provides limitation documentation

**Clinical Decision Support:**
- High-risk category
- Requires evidence of reliability
- ONTO provides reliability evidence

---

### Financial Services

**SR 11-7 (Model Risk Management):**
- Banks must validate AI models
- Epistemic calibration = model risk metric
- ONTO provides standardized validation

**SEC Regulation Best Interest:**
- AI recommendations must be suitable
- Must not overstate confidence
- ONTO measures confidence calibration

---

## ONTO as Compliance Evidence

### Documentation Provided

| Document | Regulatory Use |
|----------|----------------|
| ONTO Evaluation Report | Risk assessment evidence |
| Compliance Score | Threshold-based compliance |
| Methodology Document | Audit trail |
| Recommendations | Remediation plan |
| Historical Reports | Continuous monitoring evidence |

### Compliance Statement Template

```
AI EPISTEMIC COMPLIANCE STATEMENT

This AI system has been evaluated using ONTO-Bench, a 
standardized benchmark for epistemic calibration.

Evaluation Results:
- ONTO Compliance Score: [SCORE]/100
- Risk Level: [LEVEL]
- Unknown Detection Rate: [RATE]

This evaluation provides evidence of:
- Risk assessment per EU AI Act Article 9
- Transparency documentation per EU AI Act Article 13
- NIST AI RMF MEASURE function implementation

Evaluation ID: [ID]
Date: [DATE]
Valid for: 90 days (re-evaluation recommended)
```

---

## Positioning Against Regulatory Trends

### Current Gap

**Problem:** No industry standard for measuring AI epistemic calibration.

**Regulators asking:**
- "Does your AI know what it doesn't know?"
- "How do you measure hallucination risk?"
- "What are the documented limitations?"

**Current answers:**
- "We do internal testing" (not standardized)
- "Our accuracy is high" (wrong metric)
- "We have human oversight" (doesn't scale)

### ONTO as Standard

**Positioning:**
> "ONTO-Bench is emerging as the industry standard for epistemic calibration measurement, analogous to GLUE for NLP or ImageNet for vision."

**Differentiation:**
- Only benchmark specifically measuring epistemic boundaries
- Academic credibility (peer-reviewed methodology)
- Industry adoption (leaderboard, consortium)
- Regulatory alignment (documentation provided)

---

## Regulatory Outreach Strategy

### Phase 1: Establish Credibility (Now)

**Actions:**
1. Publish arXiv paper with regulatory relevance section
2. Submit to NIST AI standards discussions
3. Respond to EU AI Office consultations
4. Present at AI governance conferences

**Key Messages:**
- "Epistemic calibration is a measurable safety property"
- "ONTO provides standardized measurement"
- "Industry needs consistent metrics for compliance"

### Phase 2: Build Ecosystem (6 months)

**Actions:**
1. Partner with AI auditing firms
2. Engage with Big 4 (Deloitte, PwC, EY, KPMG) AI practices
3. Submit to ISO AI standards working groups
4. Seek endorsements from AI safety organizations

**Key Messages:**
- "ONTO is the de facto standard for epistemic compliance"
- "Major AI companies are adopting ONTO metrics"
- "Auditors recognize ONTO evaluations"

### Phase 3: Regulatory Recognition (12-24 months)

**Goal:** ONTO cited in regulatory guidance as acceptable measurement methodology.

**Actions:**
1. Submit formal comments on AI regulations
2. Provide expert testimony to legislators
3. Publish regulatory alignment whitepapers
4. Seek formal recognition from AI authorities

---

## Sales Messaging for Regulatory Urgency

### Email Template: Regulatory Angle

```
Subject: EU AI Act compliance for [COMPANY]

Hi [NAME],

EU AI Act is now in force. Article 9 requires documented 
risk assessment. Article 13 requires transparency about 
AI limitations.

Question: How are you documenting your AI's epistemic limits?

We provide the only standardized benchmark for measuring 
whether AI knows what it doesn't know—exactly what 
regulators are asking about.

Free pilot evaluation available. Report serves as 
compliance documentation.

Interested?

[NAME]
ONTO
```

### Call Script: Regulatory Pain

```
"Let me ask you something:

When EU AI Act enforcement ramps up next year, 
how are you planning to document your AI's limitations?

[Listen]

The challenge is there's no industry standard for this.
Everyone's making it up.

We built ONTO specifically for this—standardized measurement
of whether AI knows its limits.

The report you get from us can go directly into your 
compliance documentation.

Want to see what it looks like?"
```

### Objection: "We'll figure out compliance later"

```
"I hear that a lot. Here's the thing:

EU AI Act fines are up to 6% of global revenue.
That's not a future problem—enforcement starts 2025.

More importantly, your enterprise customers are asking NOW.
They're doing vendor risk assessments.
'How does your AI handle uncertainty?' is on the checklist.

Having ONTO documentation before they ask 
is the difference between winning and losing the deal.

Pilot takes 15 minutes. Report in 24 hours."
```

---

## Competitive Positioning

### vs. "We do internal testing"

```
"Internal testing is necessary but not sufficient.

Regulators want independent validation.
Customers want third-party certification.
Internal tests don't provide that.

ONTO gives you standardized, independent measurement
that you can show to anyone."
```

### vs. "Accuracy metrics are enough"

```
"Accuracy measures: 'Did the AI get it right?'
Epistemic calibration measures: 'Does the AI know when it CAN'T get it right?'

That's the difference between a missed answer and a lawsuit.

If your AI is 95% accurate but 100% confident,
you have a liability problem on the 5%.

We measure that specific risk."
```

### vs. "We use uncertainty quantification"

```
"Great—you're ahead of most.

Question: How do you benchmark your uncertainty?
What's the industry standard you're comparing against?

ONTO provides that standard.
You can use our benchmark to validate your uncertainty
and get independent documentation for compliance."
```

---

## Whitepaper Outline: "Epistemic Calibration and AI Regulation"

### Proposed Structure

```
1. Executive Summary
   - Regulatory landscape
   - The epistemic calibration gap
   - ONTO as solution

2. The Problem
   - AI systems don't know their limits
   - Current metrics don't capture this
   - Regulatory requirements emerging

3. Regulatory Analysis
   - EU AI Act requirements
   - NIST AI RMF alignment
   - US federal developments
   - Sector-specific (healthcare, finance)

4. ONTO Methodology
   - What we measure
   - How we measure it
   - Validation and reliability

5. Implementation Guide
   - How to use ONTO for compliance
   - Documentation provided
   - Integration with existing processes

6. Case Studies
   - Healthcare AI compliance
   - Financial services risk management
   - Enterprise AI governance

7. Conclusion
   - Call to action
   - Contact information
```

**Distribution:**
- AI policy organizations
- Regulatory affairs teams at target companies
- AI governance conferences
- LinkedIn/Twitter thought leadership

---

## Key Regulatory Quotes to Use

### EU AI Act

> "High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable users to interpret the system's output and use it appropriately."
> — Article 13, EU AI Act

**ONTO angle:** Epistemic calibration enables appropriate interpretation and use.

### NIST AI RMF

> "AI risks should be measured quantitatively or qualitatively using repeatable methods."
> — NIST AI RMF, MEASURE function

**ONTO angle:** ONTO provides repeatable quantitative measurement.

### Executive Order on AI

> "The Federal Government must ensure that AI systems are safe, secure, and trustworthy."
> — Executive Order 14110

**ONTO angle:** Epistemic calibration is a trustworthiness metric.

---

*ONTO Regulatory Positioning Pack v1.0*
*Use to create urgency and establish compliance authority*
